---
title: "Decay is inevitable, accept {linkrot}?"
author: Matt Dray
date: '2021-07-10'
slug: linkrot
categories:
  - code
  - package
tags:
  - httr
  - linkrot
  - r
  - rvest
  - xml2
draft: no
---

```{r setup, include=FALSE}
library(linkrot)
```

<div class="figure">
<img src="/post/2021-07-10-linkrot_files/404.png" alt="A screenshot of a 404 page, which is reached by following a broken link. The text says 'page not found, looks like you've followed a broken link or entered a URL that doesn't exist on this site'." width="100%"/>
</div>

# tl;dr

I wrote a function to check for [link rot](https://en.wikipedia.org/wiki/Link_rot) and put it in [the R package {linkrot}](https://github.com/matt-dray/linkrot).

# File not found

You've clicked a link before and been taken somewhere you weren't expecting. Sometimes it's because [you've been rickrolled](https://www.youtube.com/watch?v=dQw4w9WgXcQ),[^rick] sure, but content on the internet is constantly being moved or removed and links break all the time.

A hyperlink that no longer resolves can be considered to have 'rotted'. As time marches on, the 'rottenness' of the internet increases. This can be frustrating.

This blog is getting on for a hundred posts over three years. It would not be a surprise if link rot has taken hold. I think it's worth finding out if I have a problem.

# Install

Basically I need to visit every link[^link] in every post on this blog and see if it's still working.

I've written the function `detect_rot()` to do this, which I've put it in [the {linkrot} package on GitHub](https://en.wikipedia.org/wiki/Link_rot). To install:

```{r install, eval=FALSE}
install.packages("remotes")
remotes::install_github("matt-dray/linkrot")
library(linkrot)
```

In short, the `detect_rot()` function takes the URL of a web page and returns a tibble with details of each link from that page and whether it can be reached.

I've basically built it for my own amusement, so there's no guarantees.

# Example

Let's feed in the first post on this blog, from April 2018:

```{r}
url <- "https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/"
rot_df <- detect_rot(url)
```

We get a print-out of the URL and a bunch of dots. The dots are printed each time a link from the page is checked successfully.

What does the returned object look like?

```{r}
str(rot_df)
```

So, a tibble with six columns and a row per link on that page (`r nrow(rot_df)` in this example). The output tells us the URL and text of each link and also whether the page is contactable or not. 

I've recorded the special three-digit ['status code'](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) in the `response_code` column. These indicate whether a page was contactable. For example, 200 represents a typical success ('OK'), but you may be familiar with 404 ('not found') if you've found a broken link before.

We can extract any links that appear to be broken using the logical `response_success` column.

```{r}
rot_df[!rot_df$response_success, c(4, 5, 2)]
```

Uh-oh, looks like there's at least one in that post. It has [status code 404 ('client error')](https://en.wikipedia.org/wiki/HTTP_404), which basically means the thing couldn't be found.

## Blog rot

Now we know how it works for one page, lets apply the function over every post of this blog and see how many links have rotted. First we need all the post URLs:

```{r post-list, message=FALSE}
# Load packages
library(xml2)
library(rvest)
library(dplyr)
library(purrr)

# The URL of this blog's homepage
blog_url <- "https://www.rostrum.blog"

# Fetch all the links from the blog home page
blog_hrefs <- 
  read_html(blog_url) %>%  # fetch full HTML
  html_nodes("a") %>%      # nodes with links
  html_attr("href")        # the URL strings

# Complete full URLs, filter for post links only
posts <- paste0(blog_url, blog_hrefs[grepl("^/20", blog_hrefs)])
posts[1:5]  # preview
```

Now we can use {purrr} to iterate the `detect_rot()` function over the pages. By using the `map_df()` we can get a data frame as output rather than a list. I've hidden the printed output from `detect_rot()` this time because there would be nearly 100 lines of output (one per post).

```{r map-df, results='hide', cache=TRUE}
results <- map_df(posts, detect_rot)
```

So, our results tibble has `r nrow(results)` links from `r length(posts)` posts, or about `r round(nrow(results) / length(posts), 0)` links per post.

Again, we can filter the logical `response_success` column to see which links weren't successfully resolved.[^na]

```{r failures}
rotten <- filter(results, !response_success) 
sample_n(select(rotten, link_url), 5)  # random selection
```

Aha, so there were `r nrow(rotten)` of these in total, which works out to about `r round(nrow(rotten) / nrow(results), 2) * 100`% being unreachable. 

We can count the reasons for these failures by looking at the status codes.

```{r failure-count}
count(rotten, response_code, sort = TRUE)
```

You can see most of these status codes are in [the 4xx range](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#4xx_client_errors), which means 'client error'. Usually this is a problem with the link you've provided, like 404 is 'not found', 403 is 'forbidden' and 406 is 'not acceptable'.

It's hard to tell whether this level of link rot is good or bad, but remember that these are links that have failed within the past three years. Imagine how bad this might look in another 10 years.

# Under the hood

How does the function work?

You can find the function definition [in the source code](https://github.com/matt-dray/linkrot/blob/main/R/check.R). There's three steps in `detect_rot()`, each of which has [a helper function](https://github.com/matt-dray/linkrot/blob/main/R/utils.R):

1. Check that the provided URL is valid with `.validate_page()`
1. Scrape the links from the page with `.fetch_links()`
1. Visit each link and check its response code with `.check_links()`

So, the URL provided by the user is first checked with help from the {httr} package. We `GET()` the page and then extract the `status_code()` and check for an `http_error()`. If all is well (i.e. no error), then we can continue.

To get the links from the URL, we first scrape the page with `xml::read_html()` and then use {rvest} functions: `html_nodes()` to grab all the nodes with links, then `html_attr()` and `html_text()` to extract the URLs and link text from each.

Finally, each of the URLs is visited with `GET()` and the `http_status()` is extracted. The final data frame is converted to tibble (for ease of reading) and returned to the user.

# Now what?

I could go back and fix the broken links, but maybe it's not that big a deal. I don't have any data on what people click on, so I can't really tell if it's worth it.

But anyway, didn't I say 'decay is inevitable'? I can fix things, but more things will break.

I wasn't expecting this to get quite so existential.

[^rick]: Are you wondering whether to click this?
[^link]: That's links to other pages on the internet, because links also exist to take you to these footnotes, or point elsewhere internally to this website.
[^na]: As an aside, there are some links that {httr} struggled to contact. I wrapped functions internal to `detect_rot()` inside `tryCatch()` so any failures appear as `NA` in the `response_code` column. The printed output for `detect_rot()` also displays a `!` instead of a `.` when being run. There were `r nrow(filter(results, is.na(response_code)))` links that had this problem.
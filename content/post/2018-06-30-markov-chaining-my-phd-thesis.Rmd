---
title: Markov chaining my PhD thesis
author: Matt Dray
date: '2018-06-30'
slug: markov-chain-phd
categories:
  - R
  - text analysis
tags:
  - phd
  - markov chain
draft: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

<span style="color:lightgray">Matt Dray</span>

# 

I wrote a PhD thesis in 2014 called *Effects of multiple environmental stressors on litter chemical composition and decomposition* while at Cardiff University.

On graduation day, a stranger came up to me and, to paraphrase, said 'you doctors should be proud of what you've achieved, you're doing a great service'. I didn't have the heart to tell him that I wasn't a medical doctor. No, I was something nobler and altogether more unique: *a doctor of rotting leaves*.

I know you're thinking 'gosh, what a complicated subject that must be; how could I ever hope to achieve such greatness?'. My answer is that you should simply take my thesis and use a Markov chain to generate new text from it. The output will make probably as much sense as the original but won't be detected easily by plagiarism software.

Heck, I'll even do it for you in this post.

You're welcome. Don't forget to cite me.

# Text generation

I'll be using a very simple approach: Markov chains.

Basically, after providing an input dataset, a Markov chain can generate the *next* word in a sentence given the *current* word. Selection of the new word is random but weighted by occurrences in your input file.

There's [a great post on Hackernoon that explains Markov chains for text generation](https://hackernoon.com/automated-text-generator-using-markov-chain-de999a41e047). For [interactive visuals of Markov chains, go to setosa.io](http://setosa.io/ev/markov-chains/).

Text generation is an expanding field and there are much more successful and complicated methods for doing it. For example, [Andrej Karpathy generated some pretty convincing Shakespeare passages, Wikipedia pages and geometry papers in LaTeX using the 'unreasonably effective' and 'magical' power of Recurrent Neural Networks (RNNs)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).

# Code

I'll be using R code written by [Kory Becker](http://www.primaryobjects.com/) that I found [in a GitHub gist](https://gist.github.com/primaryobjects/4c7cca705eeba0d8bad6).

In a similar vein, [Roel Hogervorst](https://rmhogervorst.nl/) did a swell job of [generating Captain Picard text](http://rmhogervorst.nl/cleancode/blog/2017/01/21/content/post/2017-01-21-markov-chain/) in R from *Star Trek: The Next Generation* scripts, which is certainly [in our wheelhouse](https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/).

# Data

# Output

## Full output

I generated 200 samples frmom unique random seeds.

```{r dt}
DT::datatable()
```

## Cherry-picked phrases

# Session info

```{r session_info}
devtools::session_info()
```